{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment - Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the data into a pandas dataframe (you may get a warning, you can get rid of it by setting low_memory=False). \n",
    "\n",
    "### Print the first 10 rows and print a random sampling of the rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     status  bed bath  acre_lot           city        state  zip_code  \\\n",
      "0  for_sale  3.0  2.0      0.12       Adjuntas  Puerto Rico     601.0   \n",
      "1  for_sale  4.0  2.0      0.08       Adjuntas  Puerto Rico     601.0   \n",
      "2  for_sale  2.0  1.0      0.15     Juana Diaz  Puerto Rico     795.0   \n",
      "3  for_sale  4.0  2.0      0.10          Ponce  Puerto Rico     731.0   \n",
      "4  for_sale  6.0  2.0      0.05       Mayaguez  Puerto Rico     680.0   \n",
      "5  for_sale  4.0  3.0      0.46  San Sebastian  Puerto Rico     612.0   \n",
      "6  for_sale  3.0  1.0      0.20         Ciales  Puerto Rico     639.0   \n",
      "7  for_sale  3.0  2.0      0.08          Ponce  Puerto Rico     731.0   \n",
      "8  for_sale  2.0  1.0      0.09          Ponce  Puerto Rico     730.0   \n",
      "9  for_sale  5.0  3.0      7.46     Las Marias  Puerto Rico     670.0   \n",
      "\n",
      "   house_size prev_sold_date     price  \n",
      "0       920.0            NaN  105000.0  \n",
      "1      1527.0            NaN   80000.0  \n",
      "2       748.0            NaN   67000.0  \n",
      "3      1800.0            NaN  145000.0  \n",
      "4         NaN            NaN   65000.0  \n",
      "5      2520.0            NaN  179000.0  \n",
      "6      2040.0            NaN   50000.0  \n",
      "7      1050.0            NaN   71600.0  \n",
      "8      1092.0            NaN  100000.0  \n",
      "9      5403.0            NaN  300000.0  \n",
      "           status  bed bath  acre_lot            city          state  \\\n",
      "103636   for_sale  3.0  2.0      0.47      Burlington  Massachusetts   \n",
      "979503   for_sale  3.0  4.0      0.16       Ridgewood     New Jersey   \n",
      "1051443  for_sale  NaN  NaN      1.20  Hewlett Harbor       New York   \n",
      "1235371  for_sale  NaN  NaN     12.60  Hunter Village       New York   \n",
      "1041813  for_sale  1.0  1.0       NaN          Queens       New York   \n",
      "\n",
      "         zip_code  house_size prev_sold_date      price  \n",
      "103636     1803.0      1911.0     1992-05-29   724900.0  \n",
      "979503     7450.0         NaN     2017-11-15   900000.0  \n",
      "1051443   11557.0         NaN     2022-03-01  1199999.0  \n",
      "1235371   12442.0         NaN            NaN   449000.0  \n",
      "1041813   11372.0         NaN            NaN   475000.0  \n"
     ]
    }
   ],
   "source": [
    "#Load CSV File Into Dataframe\n",
    "df = pd.read_csv(\"data/realtor-data.csv\", low_memory=False)\n",
    "\n",
    "#Print First 10 Rows\n",
    "print(df.head(10))\n",
    "\n",
    "#Print Random Sample of Rows\n",
    "print(df.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) You should always check how many null values there are in your data as well as the data types of the data you're working with. Often you will come across data that looks correct but isn't the right data type. \n",
    "\n",
    "### Check the number of null values for every column and check the data types as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Counts:\n",
      "bed               216467\n",
      "bath              194206\n",
      "acre_lot          357467\n",
      "city                 191\n",
      "zip_code             479\n",
      "house_size        450112\n",
      "prev_sold_date    686293\n",
      "price                108\n",
      "dtype: int64\n",
      "Data Type Counts:\n",
      "object     7\n",
      "float64    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check and Print For Null Values\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"Null Counts:\")\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "#Check and Print Data Types\n",
    "data_types = df.dtypes\n",
    "data_type_counts = df.dtypes.value_counts()\n",
    "print(\"Data Type Counts:\")\n",
    "print(data_type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) We have 3 columns that looked right when checking the data but aren't the right data type and we'll correct it. \n",
    "\n",
    "### Cast the columns bed, bath and price to float. Values that cannot be casted to float, like \"hello\" should be turned into NaN. \n",
    "\n",
    "### Check the data types again to make sure the conversion was successfull.\n",
    "\n",
    "\n",
    "\n",
    "### Get a count of the number of NaNs in bed, bath and price columns. \n",
    "\n",
    "### You should get 216535, 194215 and 110 respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Data Types:\n",
      "bed      float64\n",
      "bath     float64\n",
      "price    float64\n",
      "dtype: object\n",
      "NaN Counts:\n",
      "bed      216535\n",
      "bath     194215\n",
      "price       110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Cast Bed, Bath, and Price to Floats - Or As NaN\n",
    "df['bed'] = pd.to_numeric(df['bed'], errors='coerce')\n",
    "df['bath'] = pd.to_numeric(df['bath'], errors='coerce')\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "#Check and Print Data Types Again\n",
    "print(\"New Data Types:\")\n",
    "print(df[['bed', 'bath', 'price']].dtypes)\n",
    "\n",
    "#Count and Print NaNs in Bed, Bath, and Price\n",
    "nan_counts = df[['bed', 'bath', 'price']].isnull().sum()\n",
    "print(\"NaN Counts:\")\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Check the number of unique values in the bed, bath and state columns. \n",
    "\n",
    "### You should get 49, 42 and 19 respectively\n",
    "\n",
    "### Print the uniques values for bed, bath and state. What do you notice about the unique values ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values:\n",
      "bed      49\n",
      "bath     42\n",
      "state    19\n",
      "dtype: int64\n",
      "\n",
      "Unique values in 'Bedrooms': [  3.   4.   2.   6.   5.   1.   9.  nan   7.   8.  12.  13.  10.  11.\n",
      "  33.  24.  28.  14.  18.  20.  16.  15.  19.  17.  40.  21.  86.  31.\n",
      "  27.  42.  60.  22.  32.  99.  49.  29.  30.  23.  46.  36.  68. 123.\n",
      "  25.  47.  inf  35.  38.  64.  48.  75.]\n",
      "\n",
      "Unique values in 'Bathrooms': [  2.   1.   3.   5.   4.   7.   6.  nan   8.   9.  10.  12.  13.  35.\n",
      "  11.  16.  15.  18.  20.  14.  36.  25.  17.  19.  56.  42.  51.  28.\n",
      " 198.  22.  33.  27.  30.  29.  24.  46.  21. 123.  39.  43.  32.  45.\n",
      "  64.]\n",
      "\n",
      "Unique values in 'States': ['Puerto Rico' 'Virgin Islands' 'Massachusetts' 'Connecticut'\n",
      " 'New Hampshire' 'Vermont' 'New Jersey' 'New York' 'South Carolina'\n",
      " 'Tennessee' 'Rhode Island' 'Virginia' 'Wyoming' 'Maine' 'Georgia'\n",
      " 'Pennsylvania' 'West Virginia' 'Delaware' 'Louisiana']\n"
     ]
    }
   ],
   "source": [
    "#Check and Print the Number of Unique Values in Bed, Bath, and State\n",
    "unique_counts = df[['bed', 'bath', 'state']].nunique()\n",
    "print(\"Unique values:\")\n",
    "\n",
    "print(unique_counts)\n",
    "print(\"\\nUnique values in 'Bedrooms':\", df['bed'].unique())\n",
    "print(\"\\nUnique values in 'Bathrooms':\", df['bath'].unique())\n",
    "print(\"\\nUnique values in 'States':\", df['state'].unique())\n",
    "\n",
    "#What do you notice about the unique values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) We want to see which state has the largest number of properties for sale. \n",
    "\n",
    "### Print a count of the number of properties in each state/territory. \n",
    "\n",
    "### We want to make sure that we're getting unique listings, so drop any duplicate rows and print the count of the number of properties. What do you notice about the number of properties in each state ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Properties in Each State/Territory:\n",
      "state\n",
      "New York          67160\n",
      "New Jersey        32601\n",
      "Connecticut       13753\n",
      "Massachusetts     10056\n",
      "Pennsylvania       9549\n",
      "Maine              4938\n",
      "New Hampshire      3431\n",
      "Rhode Island       3332\n",
      "Puerto Rico        2651\n",
      "Vermont            2544\n",
      "Delaware           1290\n",
      "Virgin Islands      730\n",
      "Virginia              7\n",
      "Georgia               5\n",
      "West Virginia         1\n",
      "Tennessee             1\n",
      "Wyoming               1\n",
      "South Carolina        1\n",
      "Louisiana             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Drop Duplicate Rows for Unique Listings\n",
    "df_unique = df.drop_duplicates()\n",
    "\n",
    "#Count and Print of Number of Properties in Each State/Territory\n",
    "state_counts = df_unique['state'].value_counts()\n",
    "print(\"Number of Properties in Each State/Territory:\")\n",
    "print(state_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) We now want to look for patterns in our data, find the 5 dates when the most houses were sold. What do you notice ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Dates For Mosy Houses Sold:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prev_sold_date\n",
       "2022-04-15    68\n",
       "2022-04-29    58\n",
       "2022-03-31    57\n",
       "2022-04-01    54\n",
       "2022-02-28    54\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanitize Date Sold Column Data\n",
    "df['prev_sold_date'] = pd.to_datetime(df['prev_sold_date'], errors='coerce')\n",
    "\n",
    "#Count Sales By Date and Find Top 5 Dates\n",
    "sales_by_date = df_unique['prev_sold_date'].value_counts()\n",
    "top_5_dates = sales_by_date.head(5)\n",
    "print(\"Top 5 Dates For Mosy Houses Sold:\")\n",
    "print(top_5_dates)\n",
    "\n",
    "#What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Now we want to create a simple but effective summary of the properties that are for sale. \n",
    "\n",
    "### Let's create a summary table that contains the average home size and price, every state and each city within a state. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>avg_home_size</th>\n",
       "      <th>avg_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Andover</td>\n",
       "      <td>1653.750000</td>\n",
       "      <td>2.539500e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Ansonia</td>\n",
       "      <td>1848.172414</td>\n",
       "      <td>2.917902e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Ashford</td>\n",
       "      <td>1638.888889</td>\n",
       "      <td>1.959045e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Avon</td>\n",
       "      <td>2929.878788</td>\n",
       "      <td>5.824611e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Barkhamsted</td>\n",
       "      <td>2703.538462</td>\n",
       "      <td>3.383238e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>Virgin Islands</td>\n",
       "      <td>Saint Thomas</td>\n",
       "      <td>3435.025641</td>\n",
       "      <td>1.185128e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Cape Charles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.130000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4305</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Chincoteague</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.620000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1860.000000</td>\n",
       "      <td>6.250000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Cody</td>\n",
       "      <td>1935.000000</td>\n",
       "      <td>5.350000e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4308 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               state          city  avg_home_size     avg_price\n",
       "0        Connecticut       Andover    1653.750000  2.539500e+05\n",
       "1        Connecticut       Ansonia    1848.172414  2.917902e+05\n",
       "2        Connecticut       Ashford    1638.888889  1.959045e+05\n",
       "3        Connecticut          Avon    2929.878788  5.824611e+05\n",
       "4        Connecticut   Barkhamsted    2703.538462  3.383238e+05\n",
       "...              ...           ...            ...           ...\n",
       "4303  Virgin Islands  Saint Thomas    3435.025641  1.185128e+06\n",
       "4304        Virginia  Cape Charles            NaN  7.130000e+05\n",
       "4305        Virginia  Chincoteague            NaN  1.620000e+05\n",
       "4306   West Virginia       Wyoming    1860.000000  6.250000e+04\n",
       "4307         Wyoming          Cody    1935.000000  5.350000e+05\n",
       "\n",
       "[4308 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanitize the Data for Price and Home Size\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['home_size'] = pd.to_numeric(df['house_size'], errors='coerce')\n",
    "\n",
    "#Summary of Properties For Sale\n",
    "summary_table = df_unique.groupby(['state', 'city']).agg(\n",
    "    avg_home_size=('house_size', 'mean'),\n",
    "    avg_price=('price', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "#Printed Table\n",
    "summary_table\n",
    "\n",
    "\n",
    "#Your output should be this:\n",
    "# \t\t                          house_size\tprice\n",
    "# state\t            city\t\t\n",
    "# Connecticut\t    Andover\t     1653.750000\t2.539500e+05\n",
    "#                   Ansonia\t     1848.172414\t2.917902e+05\n",
    "#                   Ashford\t     1638.888889\t1.959045e+05\n",
    "#                   Avon\t     2929.878788\t5.824611e+05\n",
    "#                   Barkhamsted\t 2703.538462\t3.383238e+05\n",
    "# ...\t...\t...\t...\n",
    "# Virgin Islands\tSaint Thomas 3435.025641\t1.185128e+06\n",
    "# Virginia\t        Cape Charles\t     NaN\t7.130000e+05\n",
    "#                   Chincoteague\t     NaN\t1.620000e+05\n",
    "# West Virginia\t    Wyoming\t     1860.000000\t6.250000e+04\n",
    "# Wyoming\t        Cody\t     1935.000000\t5.350000e+05\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
